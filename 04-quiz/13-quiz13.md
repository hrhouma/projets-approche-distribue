# Partie 3

**21. Une organisation a récemment lancé une nouvelle gamme de produits à l'échelle mondiale, ce qui a entraîné une augmentation significative des requêtes sur leur cluster Amazon Redshift.**

L'équipe d'ingénierie des données doit ajuster l'infrastructure Redshift pour gérer efficacement les requêtes concurrentes. Quelle étape permettrait de répondre aux besoins ?

- A. Activer la mise en cache des résultats dans Redshift pour réutiliser les résultats des requêtes.
- B. Recréer le cluster Redshift et activer le scaling de la concurrence dans les paramètres lors de la création.
- C. Activer le scaling de la concurrence au niveau de la file d'attente du gestionnaire de charge de travail (WLM) dans le cluster Redshift.
- D. Optimiser les clés de distribution et les clés de tri pour améliorer les performances des requêtes.

---

**22. Une entreprise de fabrication met en place une solution d'analyse de données basée sur le cloud en utilisant Amazon Redshift.**

L'entreprise doit se conformer à des exigences réglementaires strictes et souhaite documenter toutes les activités sur Redshift, y compris les requêtes SQL et les actions des utilisateurs, pour des audits. Quelle solution satisferait ces exigences avec le MOINS de surcharge opérationnelle ?

- A. Utiliser AWS Config pour surveiller en continu les fonctionnalités de journalisation de Redshift.
- B. Analyser les journaux CloudWatch depuis le cluster Redshift à l'aide d'une fonction AWS Lambda et stocker les journaux dans Amazon OpenSearch Service.
- C. Intégrer les journaux Amazon CloudWatch avec Redshift pour collecter les journaux SQL et configurer des alarmes CloudWatch.
- D. Activer la fonctionnalité de journalisation d'audit intégrée de Redshift et l'intégrer à AWS CloudTrail pour améliorer le suivi des appels API Redshift.

---

**23. Une entreprise a mis en place un pipeline de données avec un job AWS Glue qui tire des données d'un bucket S3 source, les transforme, puis envoie les résultats à un bucket S3 cible.**

Un ingénieur en données doit configurer les autorisations pour que le job Glue puisse accéder à ces buckets. Quelle option répondrait aux exigences avec le MOINS d'effort de configuration ?

- A. Utiliser des politiques de bucket Amazon S3 pour accorder l'accès à AWS Glue.
- B. Utiliser les listes de contrôle d'accès (ACL) de S3 pour gérer les autorisations.
- C. Créer un utilisateur IAM pour AWS Glue et attacher des politiques d'accès à S3.
- D. Créer un rôle IAM avec les autorisations S3 nécessaires et attribuer ce rôle au service AWS Glue.

---

**24. Une entreprise spécialisée dans l'analyse de données utilise AWS Glue pour ses tâches ETL.**

Ils rencontrent des irrégularités dans les temps d'exécution de leurs jobs et souhaitent optimiser cela. Quelle méthode leur permettrait de recueillir les données et analyses les plus détaillées pour ces opérations ?

- A. Configurer AWS X-Ray pour tracer et profiler l'exécution des jobs AWS Glue.
- B. Utiliser les événements de données d'AWS CloudTrail pour surveiller et enregistrer l'activité API des jobs ETL AWS Glue.
- C. Interroger le catalogue de données AWS Glue via Athena pour surveiller en temps réel les métriques des jobs Glue.
- D. Utiliser AWS Glue Monitoring et le profiler de jobs Glue via CloudWatch pour surveiller les opérations Glue.

---

**25. Une entreprise de commerce électronique mondiale utilise plusieurs bases de données, y compris MySQL, PostgreSQL et MongoDB.**

L'équipe d'ingénierie des données doit créer une vue unifiée de ces sources de données pour les analyses. Quelle solution répondrait à ces besoins avec le MOINS d'effort opérationnel ?

- A. Utiliser Amazon Athena Federated Query pour déployer et configurer les connecteurs de source de données appropriés pour chaque moteur de base de données, puis exécuter des requêtes avec Athena.
- B. Exécuter un code personnalisé avec une fonction AWS Lambda pour extraire les données des bases de données, stocker les données extraites dans DynamoDB et utiliser Amazon Athena pour interroger.
- C. Utiliser AWS Data Pipeline pour transférer les données de chaque base de données vers Amazon Redshift et interroger les données avec SQL.
- D. Utiliser Amazon EMR pour exécuter des frameworks de big data comme Apache Spark ou Hadoop sur les données de ces bases de données.

---

**26. Une entreprise possède un grand volume de données d'achats stockées dans une instance Amazon RDS.**

Ces données sont sensibles et nécessitent un contrôle d'accès strict. L'entreprise souhaite également permettre des requêtes complexes pour des analyses commerciales. Quelle solution répondrait aux besoins avec le MOINS de surcharge opérationnelle ?

- A. Transférer les données vers Amazon Redshift pour l'analyse et utiliser les groupes de sécurité AWS pour le contrôle d'accès.
- B. Utiliser directement Amazon RDS pour les requêtes complexes et mettre en œuvre des contrôles d'accès RDS.
- C. Migrer les données vers un bucket Amazon S3 et utiliser IAM pour le contrôle d'accès, avec Amazon Athena pour les requêtes et analyses.
- D. Utiliser AWS Lake Formation pour configurer un lac de données et connecter RDS via AWS Glue.

---

**27. Un ingénieur en données doit régulièrement mettre à jour l'entrepôt de données Amazon Redshift avec des lots de nouvelles données chaque mois.**

Ces données contiennent souvent des mises à jour pour des enregistrements existants. Quelle stratégie devrait être utilisée pour assurer une ingestion efficace tout en minimisant l'impact sur les performances des requêtes ?

- A. Charger les données dans une table temporaire avec INSERT, puis exécuter un job Glue qui applique la transformation Drop Duplicates.
- B. Utiliser la commande INSERT pour charger les données dans une table temporaire, puis utiliser UPSERT pour insérer et mettre à jour les enregistrements dans la table principale.
- C. Créer une table temporaire et utiliser MERGE pour insérer les nouvelles données et mettre à jour les enregistrements existants dans la table principale.
- D. Charger les données dans une table temporaire avec INSERT, puis utiliser DELETE pour supprimer les enregistrements dans la table principale avant d'insérer les nouvelles données.

---

**28. Un ingénieur en données maintient plusieurs jobs ETL dans AWS Glue.**

Parmi ces jobs, il existe une tâche non urgente qui traite de grandes quantités de données. Quelle solution permettrait d'optimiser les coûts d'exécution de ce job sans compromettre les performances ?

- A. Utiliser l'exécution STANDARD d'AWS Glue pour le job.
- B. Utiliser AWS Glue Flex pour le job.
- C. Utiliser des instances spot pour le job.
- D. Utiliser une plus petite unité de traitement des données (DPU).

---

**29. Une entreprise de vente au détail utilise AWS pour le traitement et l'analyse de données.**

Ils ont leurs ensembles de données principaux stockés dans un lac de données S3, mais aussi plusieurs bases de données comme Aurora MySQL, HBase sur EMR, et DynamoDB. L'entreprise a besoin de générer des rapports financiers complets en interrogeant et combinant ces sources sans déplacer ou dupliquer les données. Quelle solution AWS permettrait de réaliser ces requêtes efficacement ?

- A. Utiliser AWS Glue pour l'extraction, la transformation et le chargement (ETL) des données dans S3, puis interroger avec Athena.
- B. Configurer Amazon QuickSight pour interroger et visualiser directement les données depuis plusieurs sources.
- C. Utiliser Amazon Athena Federated Query pour exécuter des requêtes SQL sur les données dans S3, Aurora MySQL, HBase et DynamoDB sans déplacement de données.
- D. Configurer Amazon Redshift Spectrum pour interroger les données de plusieurs sources et stocker les résultats dans S3.

---

**30. Une entreprise développe un pipeline de traitement des données pour sa plateforme de commerce électronique.**

Le pipeline nécessite l'exécution d'un code Python chaque lundi à 9h00 UTC pour analyser les données de comportement des clients stockées dans des buckets S3. Quelle(s) option(s) conviendrait le mieux à ce scénario ? (SÉLECTIONNER DEUX)

- A. Utiliser Amazon EventBridge pour exécuter le code Python chaque lundi à 9h00 UTC.
- B. Utiliser AWS Cloud9 pour écrire, exécuter et déboguer le code Python.
- C. Utiliser AWS CloudShell pour exécuter le code Python.
- D. Utiliser AWS Cloud Development Kit (CDK) pour définir l'infrastructure et la provisionner via AWS CloudFormation.
- E. Utiliser AWS Lambda pour exécuter le code Python.

