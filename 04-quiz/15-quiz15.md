# Partie 5

**41. Une équipe d'ingénierie des données automatise le processus d'extraction, de transformation et de chargement (ETL) de fichiers JSON compressés en gzip à partir d'un bucket Amazon S3.**

Les fichiers gzip sont téléchargés quotidiennement et chaque fichier a une taille totale inférieure à 30 Mo. L'objectif est de mettre en place le pipeline ETL de la manière la plus rentable. Quelle action devrait être entreprise pour répondre à cette exigence ?

- A. Utiliser AWS Glue for Ray pour écrire des scripts Python pour la transformation des données.
- B. Utiliser AWS Glue sur Apache Spark, en écrivant la logique en Scala pour la transformation des données.
- C. Utiliser AWS Glue sur Apache Spark, en utilisant PySpark pour la transformation des données.
- D. Utiliser un job AWS Glue avec Python Shell et pandas pour la transformation des données.

---

**42. Une entreprise de fabrication gère des dispositifs IoT qui collectent et stockent des enregistrements dans Amazon Timestream et Amazon Redshift.**

L'analyse nécessite de combiner ou d'effectuer des jointures sur les données des deux bases de données. Quelle solution répond aux besoins avec le MOINS de surcharge opérationnelle ?

- A. Utiliser AWS Athena Federated Query pour exécuter des requêtes SQL qui couvrent à la fois Amazon Timestream et Redshift.
- B. Exporter les données d'Amazon Timestream vers Amazon S3, puis utiliser la commande COPY pour les charger dans la table Amazon Redshift.
- C. Utiliser Redshift Federated Query pour accéder aux données de Timestream et les interroger avec les tables Redshift.
- D. Utiliser Redshift Spectrum pour interroger les données dans Timestream et les joindre avec les tables Redshift.

---

**43. Une entreprise utilise Amazon Redshift pour ses besoins en entreposage de données.**

L'équipe d'ingénierie des données doit exécuter une analyse exploratoire des données (EDA) et exporter les résultats pour un traitement ultérieur. Les détails de connexion du cluster Redshift doivent être stockés et récupérés en toute sécurité. Quelle approche serait appropriée ? (SÉLECTIONNER DEUX)

- A. Utiliser Amazon SageMaker Data Wrangler pour interroger les informations nécessaires à partir du cluster Redshift et exporter les résultats vers S3 pour traitement.
- B. Configurer AWS Secrets Manager pour stocker les détails de connexion du cluster Redshift et utiliser AWS SDK for pandas pour récupérer les valeurs secrètes.
- C. Lancer un cluster Amazon EMR et exécuter un job Apache Spark pour interroger les informations pertinentes du cluster Redshift. Exporter les résultats vers S3.
- D. Utiliser AWS Lambda pour exécuter les requêtes EDA sur le cluster Redshift et stocker les résultats dans DynamoDB.
- E. Configurer AWS Systems Manager Parameter Store pour stocker les détails de connexion du cluster Redshift et utiliser l'API Amazon Redshift Data pour se connecter au cluster.

---

**44. Une entreprise doit créer un pipeline de données qui extrait des données d'une instance DB Amazon RDS, les transforme avec AWS Glue, et les charge dans un cluster Amazon Redshift.**

Les données sont mises à jour quotidiennement à minuit et doivent être disponibles dans Redshift avant 2h du matin. Quelle solution répondrait le mieux aux besoins ? (SÉLECTIONNER DEUX)

- A. Configurer un job AWS Glue ETL pour transformer les données et un déclencheur Glue pour démarrer le job quotidiennement après la mise à jour à minuit.
- B. Utiliser AWS Glue pour créer un lac de données contenant les données brutes de l'instance RDS, puis interroger le lac de données avec Amazon Athena.
- C. Créer une connexion AWS Glue pour l'instance Amazon RDS et le cluster Redshift, puis utiliser Amazon EMR pour transformer les données.
- D. Configurer un job AWS Glue ETL pour transformer les données et un déclencheur Glue pour démarrer le job toutes les heures.
- E. Créer une connexion AWS Glue pour l'instance RDS et le cluster Amazon Redshift.

---

**45. Une entreprise développe une application d'analyse de données qui doit récupérer et traiter des données depuis un bucket S3.**

Les modèles d'accès aux données sont complexes et varient en fonction de l'analyse. Quelle solution AWS devrait être utilisée pour permettre à l'entreprise d'utiliser son propre code pour traiter et transformer les données lorsqu'elles sont récupérées de S3 ?

- A. AWS Glue
- B. AWS Lambda
- C. Amazon S3 Storage Lens
- D. Amazon S3 Object Lambda

---

**46. Une entreprise utilise un flux de données Amazon Kinesis pour gérer ses workloads de traitement de données en temps réel.**

L'application consommatrice a commencé à rencontrer des erreurs `ProvisionedThroughputExceededException`. Quelles actions devraient être entreprises pour résoudre ce problème ? (SÉLECTIONNER DEUX)

- A. Activer la fonctionnalité de surveillance améliorée (enhanced monitoring) pour le flux.
- B. Implémenter une logique de backoff exponentiel et de nouvelles tentatives dans l'application.
- C. Augmenter le nombre de shards dans le flux Kinesis.
- D. Activer la fonctionnalité de fan-out amélioré (enhanced fan-out) sur le flux.
- E. Utiliser un type d'instance EC2 plus grand pour l'application Kinesis.

---

**47. Un développeur souhaite utiliser AWS Lambda pour manipuler des données dans une instance RDS PostgreSQL dans un sous-réseau privé.**

Le développeur doit se connecter à la base de données au port 5432 sans traverser l'internet public. Quelle solution répondrait aux besoins avec le MOINS de surcharge opérationnelle ?

- A. Migrer l'instance RDS vers un sous-réseau public. Configurer Lambda pour se connecter avec des identifiants stockés dans Secrets Manager.
- B. Déployer Lambda dans le même sous-réseau privé. Autoriser le trafic depuis le groupe de sécurité de Lambda via le port 5432 dans le groupe de sécurité de RDS.
- C. Activer l'authentification IAM DB dans l'instance RDS. Créer un rôle IAM permettant l'accès à RDS et l'attacher à Lambda.
- D. Déployer Lambda dans un VPC différent. Autoriser le trafic depuis le groupe de sécurité de Lambda via le port 5432 dans le groupe de sécurité de RDS.

---

**48. Une entreprise utilise une fonction Lambda pour récupérer des objets d'un bucket S3 dans le même compte AWS.**

La politique IAM actuelle du rôle d'exécution de Lambda est trop permissive. Quelle modification serait la plus appropriée pour adhérer au principe du moindre privilège ?

- A. Ajouter `Action": ["s3:GetObjectVersion"]` et remplacer `Resource` par `"arn:aws:s3:::bucket-name/*"`.
- B. Remplacer `Action` par `["s3:GetObject"]` et `Resource` par `"arn:aws:s3:::bucket-name/*"`.
- C. Ajouter `Action": ["s3:ListBucket"]` et remplacer `Resource` par `"arn:aws:s3:::bucket-name"`.
- D. Remplacer `Action` par `["s3:PutObject"]` et `Resource` par `"arn:aws:s3:::bucket-name/*"`.

---

**49. Une entreprise qui fournit un service de géolocalisation collecte des jeux de données de coordonnées longitude et latitude dans des fichiers CSV pouvant atteindre 3 Go, stockés dans S3.**

L'équipe d'ingénierie des données doit fusionner les champs de latitude et longitude, puis analyser les données pour trouver le nombre total d'entrées clients uniques. Comment l'équipe peut-elle satisfaire cette exigence avec le MOINS d'effort de développement ?

- A. Construire une recette avec AWS Glue DataBrew pour calculer le nombre distinct de clients en utilisant la fonction `COUNT_DISTINCT`.
- B. Écrire un script PySpark pour compter les entrées distinctes et l'exécuter dans une application EMR Serverless.
- C. Utiliser AWS Glue Crawlers pour déduire le schéma des données sources et écrire un job Spark AWS Glue pour effectuer le comptage des clients uniques.
- D. Configurer une fonction AWS Lambda pour exécuter un script Python qui lit le fichier S3, traite la concaténation et utilise un algorithme de comptage distinct.

---

**50. Une entreprise prépare le déploiement d'un cluster Amazon EMR pour des tâches d'extraction, transformation et chargement (ETL) avec Apache Spark.**

Le cluster doit être fiable sous de lourdes charges de travail et stocker les données de manière persistante. Quelle combinaison de stockage et de calcul répondrait à ces besoins de la manière la plus rentable ?

- A. Utiliser Amazon Elastic File System (EFS) comme stockage et des instances spot pour le calcul.
- B. Utiliser Hadoop Distributed File System (HDFS) comme stockage et des instances x86 pour le calcul.
- C. Utiliser le système de fichiers local comme stockage et des instances spot pour le calcul.
- D. Utiliser Elastic Map Reduce File System (EMRFS) comme stockage et des instances Graviton pour le calcul.

