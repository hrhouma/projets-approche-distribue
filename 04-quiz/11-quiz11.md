# Partie 1

**1. Une équipe d'ingénieurs en données améliore l'observabilité d'une application déployée dans un cluster Amazon Elastic Kubernetes Service (Amazon EKS).**

L'équipe souhaite centraliser la collecte des métriques système de bas niveau et des journaux de divers microservices, tout en permettant la visualisation en quasi-temps réel de ces données pour une meilleure compréhension et une réponse plus rapide.

Quelle solution satisfera ces exigences avec le MOINS de surcharge opérationnelle ?

- A. Configurer un agent Amazon Kinesis pour canaliser les journaux des microservices vers Amazon Kinesis Data Streams. Relayer les journaux vers Amazon DynamoDB avec AWS Lambda pour le stockage et l'analyse.
- B. Collecter des métriques et des journaux des microservices en utilisant Amazon CloudWatch Container Insights. Configurer un filtre de souscription de Data Firehose pour transmettre les journaux à un cluster Amazon OpenSearch Service.
- C. Utiliser Amazon CloudWatch Application Insights pour agréger les métriques et journaux des microservices. Configurer un filtre de souscription AWS Lambda pour pousser les données de journaux vers un cluster Amazon OpenSearch Service.
- D. Canaliser les journaux des microservices vers Data Firehose, puis relayer les journaux vers OpenSearch Service pour une recherche et une analyse améliorées.

---

**2. Une entreprise utilise AWS Glue pour les tâches ETL et Athena pour interroger les données stockées dans S3.**

L'entreprise a des politiques de sécurité strictes et nécessite un contrôle d'accès fin sur les ressources AWS Glue. Il existe deux départements, Finance et RH, avec leurs propres tâches ETL et bases de données.

Quelle approche serait la plus appropriée pour mettre en œuvre cette exigence ?

- A. Utiliser les politiques de bucket Amazon S3 pour contrôler l'accès aux ressources AWS Glue.
- B. Utiliser les paramètres de chiffrement du catalogue de données AWS Glue pour contrôler l'accès.
- C. Utiliser des groupes de travail Amazon Athena pour contrôler l'accès aux ressources AWS Glue.
- D. Utiliser les politiques de ressources du catalogue de données AWS Glue pour gérer l'accès à un niveau granulaire.

---

**3. Une entreprise a créé un pipeline ETL avec Amazon Glue qui charge des données de S3 dans une table Redshift. Le travail Glue utilise la commande TRUNCATE pour actualiser la table.**

Cependant, ils rencontrent des blocages sur les jobs en raison de verrous d'accès inattendus. Quelle(s) étape(s) peut utiliser un ingénieur en données pour résoudre ce problème ? (SÉLECTIONNER DEUX)

- A. Utiliser la commande EXPLAIN pour analyser et optimiser le plan d'exécution de la requête pour éviter les verrous.
- B. Utiliser la commande VACUUM pour nettoyer la table et libérer le verrou.
- C. Utiliser la commande REINDEX sur la table pour reconstruire les index et tenter de supprimer le verrou.
- D. Exécuter une requête pour identifier les sessions tenant les verrous.
- E. Utiliser pg_terminate_backend pour terminer la session qui détient le verrou.

---

**4. Un ingénieur en données doit intégrer un grand volume de données clients dans un lac de données Amazon S3.**

Il est impératif de détecter et de masquer les informations personnelles identifiables (PII) avant leur ingestion. Quelle option répond le mieux à cette exigence ?

- A. Utiliser la transformation ML FindMatches d'AWS Glue pour détecter les données sensibles.
- B. Utiliser les classificateurs intégrés d'AWS Glue pour détecter les données sensibles.
- C. Utiliser Amazon Macie pour détecter les données sensibles.
- D. Utiliser la fonctionnalité de détection de données sensibles d'AWS Glue.

---

**5. Une équipe de développement gère plusieurs tâches AWS Glue via la console Glue.**

Ils souhaitent héberger le code source des jobs dans un dépôt pour suivre les modifications et s'assurer que celles-ci soient automatiquement reflétées dans les tâches Glue.

Quelle solution répondra à ces besoins avec le MOINS d'effort de développement ?

- A. Utiliser AWS CodeCommit et AWS CodePipeline pour l'intégration et la livraison continues.
- B. Utiliser AWS CodeCommit pour le contrôle de version et AWS CodeBuild pour l'intégration continue et la livraison.
- C. Utiliser GitHub pour le contrôle de version et AWS CodeCommit pour l'intégration continue et la livraison.
- D. Utiliser GitHub pour le contrôle de version et la fonctionnalité de contrôle de version de AWS Glue Studio pour synchroniser les modifications.

---

**6. Un ingénieur en données travaille pour une entreprise qui utilise intensivement l'analyse des données volumineuses avec un cluster Amazon EMR exécutant Apache Hive.**

De nouvelles données sont ajoutées directement à HDFS mais ne sont pas visibles dans Hive. Quelle action prendre pour les rendre visibles ?

- A. Exécuter la commande MSCK REPAIR TABLE.
- B. Exécuter ALTER TABLE pour supprimer et recréer la partition.
- C. Supprimer et télécharger de nouveau les données sur HDFS.
- D. Redémarrer le cluster EMR.

---

**7. Une entreprise utilise Amazon Redshift pour ses besoins d'entreposage de données liés à une application de commerce électronique.**

Ils ont besoin d'accélérer les requêtes SQL sur les données stockées à la fois dans Redshift et S3. Quelle est la solution la plus efficace ?

- A. Utiliser une opération JOIN SQL avec Redshift Spectrum pour fusionner les données récentes avec les données historiques dans S3.
- B. Exécuter la commande VACUUM DELETE dans Redshift puis charger les données de S3.
- C. Utiliser UNLOAD pour déplacer les données dans une table externe sur S3 et interroger via Spectrum.
- D. Créer une vue matérialisée dans Redshift pour combiner les résultats de requêtes S3.

---

**8. Une équipe d'ingénieurs en données rencontre des latences élevées dans un pipeline qui utilise EMR et Athena.**

Ils constatent que le format des données n'est pas optimisé pour Athena. Quelle est la cause la plus probable ?

- A. Le cluster EMR se trouve dans une région différente d'Athena.
- B. Le bucket S3 où les données sont stockées est dans une région différente du cluster EMR.
- C. Le format des données interrogeables par Athena n'est pas en format columnaire.
- D. La limite d'exécution des requêtes Athena a été atteinte.

---

**9. Un ingénieur en données développe un service de traitement de données en Python.**

Il doit être exposé sous forme d'API REST privées avec de faibles latences. Quelle option déployer avec le MOINS d'effort ?

- A. Utiliser un cluster Amazon ECS avec API Gateway.
- B. Déployer sur Amazon EKS et configurer API Gateway pour acheminer les requêtes.
- C. Utiliser des fonctions AWS Lambda avec provisionnement de capacité pour le service, et API Gateway pour les appels.
- D. Utiliser des fonctions Lambda avec SnapStart et les intégrer à API Gateway.

---

**10. Une équipe d'ingénieurs en données dans une entreprise médiatique développe une nouvelle fonctionnalité permettant aux utilisateurs de télécharger des vidéos.**

Quelle est la solution la plus efficace pour transcodage après upload ?

- A. Configurer Amazon S3 pour publier un événement vers une file SQS.
- B. Configurer des notifications S3 pour déclencher une fonction AWS Lambda.
- C. Utiliser Amazon Kinesis Data Streams pour transmettre les données de S3 à Lambda.
- D. Utiliser Lambda pour surveiller le bucket S3 et déclencher le service de transcodage.

