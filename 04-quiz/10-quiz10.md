# Partie 1

### **Question 1 :**

Une équipe d'analystes de données dans une grande entreprise de vente au détail exécute fréquemment des requêtes complexes sur Amazon Athena pour générer des rapports hebdomadaires de ventes. Ces rapports sont essentiels pour leurs stratégies de marketing et de gestion des stocks. Les données pertinentes pour ces rapports ne changent pas durant la période de génération du rapport. L'équipe explore des méthodes pour optimiser les performances des requêtes et réduire les coûts tout en maintenant la précision et la rapidité des rapports.

**Quelle approche l'équipe devrait-elle adopter pour atteindre ces objectifs tout en interrogeant leurs données de vente sur Amazon Athena ?**

1. Utiliser Amazon Redshift pour stocker les données de ventes hebdomadaires et exécuter les requêtes à partir de là.
2. Activer la fonctionnalité « Réutiliser les résultats de requête » pour chaque requête et définir une durée maximale de 7 jours pour la réutilisation des résultats.
3. Modifier légèrement la requête à chaque fois pour s'assurer que les résultats sont frais pour des rapports de ventes hebdomadaires précis.
4. Définir une limite maximale de scan de données par requête de 1 Go dans Amazon Athena.

---

### **Question 2 :**

Une entreprise possède un grand volume de données d'achats stockées dans une instance Amazon RDS. Ces données sont sensibles et nécessitent un contrôle d'accès strict, y compris un contrôle d'accès granulaire entre comptes. Cependant, l'entreprise souhaite également faciliter l'exécution de requêtes complexes et l'analyse de ces données à des fins d'intelligence d'affaires.

**Laquelle des options suivantes répond aux exigences avec le MOINS de surcharge opérationnelle ?**

1. Utiliser directement Amazon RDS pour exécuter des requêtes complexes et des analyses. Mettre en place des contrôles d'accès RDS pour limiter l'accès.
2. Transférer les données vers Amazon Redshift pour les analyser. Utiliser des groupes de sécurité AWS pour contrôler l'accès.
3. Exploiter un lac de données via AWS Lake Formation. Configurer une connexion JDBC à Amazon RDS en utilisant AWS Glue. Enregistrer votre bucket S3 dans Lake Formation. Mettre en œuvre les contrôles d'accès fournis par Lake Formation pour limiter l'accès.
4. Migrer les données vers un bucket Amazon S3 et utiliser AWS IAM pour contrôler l'accès. Utiliser Amazon Athena pour l'exécution des requêtes et les analyses.

---

### **Question 3 :**

Une entreprise de logistique gère un lac de données Amazon S3. Ce lac de données est régulièrement mis à jour avec de nouvelles informations de suivi des expéditions, stockées dans un format partitionné compatible Hive. L'entreprise utilise Amazon Athena pour analyser ces données. Cependant, l'équipe d'ingénierie des données a récemment rencontré un problème où les requêtes Athena ne renvoient pas de résultats à partir des nouvelles partitions de données ajoutées dans le lac de données S3.

**Comment l'équipe peut-elle s'assurer qu'Athena renvoie les données nouvellement insérées et les dernières partitions avec un effort de développement minimal ?**

1. Exécuter la commande **MSCK REPAIR TABLE** dans Athena pour découvrir et ajouter automatiquement les nouvelles partitions compatibles Hive à partir du lac de données S3.
2. Mettre à jour manuellement les métadonnées de la table Athena pour chaque nouvelle partition ajoutée au lac de données S3.
3. Utiliser un AWS Glue Crawler programmé pour s'exécuter quotidiennement, qui détectera et mettra automatiquement à jour Athena avec les nouvelles partitions dans le lac de données S3.
4. Utiliser la commande **CREATE TABLE AS SELECT (CTAS)** dans Athena pour répliquer périodiquement les données dans de nouvelles tables avec des partitions mises à jour.

---

### **Question 4 :**

Une équipe d'ingénierie des données consolide les dossiers de patients dans un lac de données Amazon S3. Certains de ces dossiers ont été écrits à la main et numérisés via la reconnaissance optique de caractères (OCR). L'équipe souhaite s'assurer de la qualité des données avant de les intégrer à son pipeline ETL. Des vérifications importantes, telles que la validation du format des données et la détection d'erreurs typographiques, doivent être effectuées.

**Quel service AWS fournira le MOINS de surcharge de développement ?**

1. Utiliser AWS Glue Data Quality pour surveiller et valider automatiquement la qualité des données au sein du pipeline ETL. Configurer des règles de validation et des métriques dans AWS Glue pour vérifier la correction du format, la duplication et l'intégrité.
2. Implémenter Amazon Athena pour interroger et analyser les données dans le lac de données Amazon S3.
3. Créer des workflows de traitement des données dans Amazon EMR pour traiter et valider la qualité des données à l'aide de scripts personnalisés.
4. Configurer AWS Data Pipeline pour gérer et automatiser le mouvement et la transformation des données. Intégrer des scripts de validation personnalisés pour vérifier le format des données et détecter les duplications.

---

### **Question 5 :**

Une équipe d'analyse de données travaille avec un grand ensemble de données historiques sur les tremblements de terre, stocké dans un bucket Amazon S3. Leur objectif est de déterminer la répartition des tremblements de terre à travers différentes régions géographiques. Pour ce faire, ils doivent calculer des zones de grille basées sur les coordonnées en utilisant un système d'indexation géospatiale.

**Quelle est la méthode la plus efficace pour l'équipe d'incorporer ce calcul dans leurs requêtes Athena ?**

1. Utiliser Amazon Redshift pour prétraiter et catégoriser les données en fonction du système d'indexation géospatiale avant de les interroger avec Athena.
2. Implémenter une fonction définie par l'utilisateur (UDF) dans Amazon Athena, alimentée par AWS Lambda, pour calculer les zones de grille à l'aide du système d'indexation géospatiale.
3. Incorporer directement la logique de calcul dans chaque requête SQL dans Athena.
4. Catégoriser manuellement chaque enregistrement de tremblement de terre en fonction du système de grille avant de stocker les données dans le bucket S3.

---

### **Question 6 :**

Une entreprise de vente au détail utilise AWS pour le traitement et l'analyse de données à grande échelle. Les ensembles de données principaux sont stockés dans un lac de données Amazon S3. Cependant, l'entreprise gère également plusieurs bases de données contenant des données financières critiques dans divers formats et emplacements, notamment une base de données Aurora MySQL, une base de données HBase sur Amazon EMR, et une base de données DynamoDB. L'entreprise a besoin de créer des rapports financiers complets en interrogeant et combinant les données de ces différentes sources sans déplacer ou dupliquer les données dans S3.

**Quel service AWS l'équipe d'ingénierie des données peut-elle utiliser pour effectuer efficacement ces requêtes sur différentes sources de données ?**

1. Configurer Amazon QuickSight pour interroger directement et visualiser les données à partir de diverses sources.
2. Utiliser Amazon Athena Federated Query pour exécuter des requêtes SQL sur les données stockées dans S3, Aurora MySQL, HBase sur EMR et DynamoDB sans déplacement de données.
3. Utiliser AWS Glue pour extraire, transformer et charger (ETL) les données de toutes les sources dans Amazon S3, puis les interroger en utilisant Amazon Athena.
4. Configurer Amazon Redshift Spectrum pour interroger les données provenant de plusieurs sources et stocker les résultats dans S3.

---

### **Question 7 :**

Une organisation de santé regroupe des données de patients provenant de diverses sources dans un bucket de staging sur Amazon S3. Ces données sont ensuite traitées par un travail AWS Glue ETL et stockées dans un lac de données, également situé sur Amazon S3. Les données stockées sont accessibles pour l'interrogation via Amazon Athena.

**Quelle solution permettrait à l'organisation de faire correspondre les dossiers des patients avec le MOINS de surcharge de développement ?**

1. Mettre en place un processus de filtrage en utilisant la classe de filtrage PySpark d'AWS Glue qui itère à travers les données des patients en fonction de critères prédéfinis pour regrouper les dossiers qui partagent des attributs communs.
2. Configurer la transformation de machine learning FindMatches d'AWS Glue en l'ajoutant au travail AWS Glue et en utilisant un fichier d'étiquetage pour guider l'appariement. Utiliser les données étiquetées pour améliorer de manière itérative la capacité de la transformation à identifier et regrouper des dossiers similaires de patients.
3. Créer un script AWS Glue personnalisé en utilisant Python qui compare itérativement les dossiers des patients en fonction de motifs RegEx prédéfinis pour identifier les correspondances à travers l'ensemble de données.
4. Configurer AWS Glue Data Quality pour analyser les motifs dans les dossiers des patients en définissant des règles qui identifient des correspondances potentielles. Comparer les champs identifiables et signaler les dossiers avec des scores de similarité élevés.

---

### **Question 8 :**

Une équipe d'ingénierie des données dans une grande entreprise de commerce électronique travaille à améliorer les performances de leurs charges de travail analytiques. Ils utilisent extensivement Amazon Athena pour interroger un large éventail de données provenant de diverses plateformes et consolidées dans Amazon S3.

**Laquelle des méthodes suivantes serait la PLUS efficace pour optimiser les performances des requêtes pour leurs besoins spécifiques ?**

1. Mettre à jour fréquemment l'indexation des données dans Amazon S3 afin d'améliorer la vitesse de récupération des données pour les requêtes Amazon Athena.
2. Utiliser une requête fédérée dans Amazon Athena pour interroger simultanément des données provenant de plusieurs sources disparates.
3. Activer la fonctionnalité de réutilisation des résultats de requêtes d'Amazon Athena pour les requêtes fréquemment exécutées.
4. Exécuter une requête **CREATE TABLE AS SELECT (CTAS)** dans Amazon Athena pour convertir les résultats de requêtes dans des formats de stockage efficaces comme Parquet ou ORC.

---

### **Question 9 :**

Une entreprise de marketing digital gère un lac de données sur Amazon S3. L'entreprise utilise AWS Glue Data Catalog pour la gestion des métadonnées. Au fil des années, le lac de données a considérablement augmenté en raison de l'ajout de diverses sources de données et de métadonnées.

**Quelle solution permettra à l'administrateur de gérer les permissions avec le MOINS de surcharge de développement ?**

1. Appliquer des politiques de bucket S3 pour un contrôle intégré des permissions à la fois sur AWS Glue et Amazon S3.
2. Utiliser la fonctionnalité de planification de tâches d'AWS Glue DataBrew pour aligner les droits d'accès entre Amazon S3 et AWS Glue Data Catalog.
3. Déployer AWS Glue Data Quality pour gérer les contrôles d'accès via des règles de validation dictant les permissions pour Amazon S3 et AWS Glue Data Catalog.
4. Utiliser les permissions AWS Lake Formation pour gérer les politiques d'accès à la fois pour Amazon S3 et AWS Glue Data Catalog.

---

### **Question 10 :**

Une entreprise d'analytique dans le domaine de la santé cherche à améliorer ses pratiques de gestion des données. Cette entreprise traite une vaste gamme de types de données, notamment des dossiers de patients, des images médicales et des informations de recherche. L'objectif est de compiler et d'organiser ces données de manière cohérente afin de les rendre facilement disponibles pour l'analyse par des membres de l'équipe ayant des niveaux de compétences techniques variés. Cela inclut des exigences à la fois pour la personnalisation basée sur des scripts et pour une interface visuelle pour une facilité d'utilisation.

**Laquelle des options suivantes peut répondre à ces exigences ?**

1. Mettre en œuvre des tâches Python Shell dans AWS Glue pour l'exécution de scripts ETL personnalisés en Python.
2. Utiliser AWS Glue DataBrew pour la préparation visuelle des données.
3. Utiliser AWS Glue Data Catalog pour organiser et gérer les métadonnées des actifs de données.
4. Utiliser AWS Glue Studio pour concevoir, exécuter et surveiller les tâches ETL via une interface graphique.


