# Partie 4

**31. Une grande entreprise de commerce électronique utilise un flux Amazon Kinesis pour ingérer des données en temps réel de son site Web, telles que les clics d'utilisateurs et les vues de pages.**

Un événement de vente majeur a entraîné une augmentation significative du volume de données, et le flux Kinesis ne parvient pas à gérer cette augmentation, entraînant des pertes de données et une latence accrue. Quelles actions doivent être prises pour gérer cette augmentation du volume de données ? (SÉLECTIONNER DEUX)

- A. Augmenter le nombre de shards dans le flux Kinesis pour gérer le volume de données accru en utilisant la commande SplitShard.
- B. Réduire le nombre de shards dans le flux Kinesis pour réduire la complexité en utilisant la commande MergeShard.
- C. Remplacer le flux de données par Amazon Data Firehose.
- D. Activer Kinesis Data Streams Enhanced Fan-Out et la récupération de données HTTP/2 pour réduire la latence.
- E. Mettre en œuvre le scaling pas à pas (Step Scaling) pour le flux Kinesis.

---

**32. Une organisation génère 100 Mo de journaux de données chaque jour, initialement stockés dans Amazon S3 Standard.**

Les journaux sont fréquemment consultés pendant les 2 premiers mois, puis occasionnellement pendant les 10 mois suivants, et rarement après cela. Cependant, l'organisation souhaite conserver ces données pendant 7 ans pour des raisons de conformité. Quelle politique de cycle de vie S3 doit être mise en œuvre ?

- A. Après 2 mois, passer à S3 Standard-Infrequent Access (S3 Standard-IA), puis passer à S3 One Zone-Infrequent Access (S3 One Zone-IA) après 10 mois, et enfin supprimer les données après 7 ans.
- B. Après 2 mois, passer à S3 Standard-Infrequent Access (S3 Standard-IA), puis passer à S3 Glacier Flexible Retrieval après 10 mois, et enfin supprimer les données après 7 ans.
- C. Après 2 mois, passer à S3 One Zone-Infrequent Access (S3 One Zone-IA), puis passer à S3 Glacier Flexible Retrieval après 10 mois, et enfin supprimer les données après 7 ans.
- D. Après 2 mois, passer à S3 Standard-Infrequent Access (S3 Standard-IA), puis passer à S3 Glacier Deep Archive après 10 mois, et enfin supprimer les données après 7 ans.

---

**33. Une organisation utilise un bucket Amazon S3 dans la région us-east-1 pour stocker des documents légaux et souhaite enregistrer toutes les opérations d'écriture effectuées sur ce bucket dans un autre bucket S3 dans la même région.**

Quelle solution répondrait à ces exigences avec le MOINS d'effort opérationnel ?

- A. Dans AWS CloudTrail, configurer une piste qui enregistre uniquement les événements de données d'écriture du bucket S3 source et diriger ces journaux vers le bucket S3 de destination.
- B. Suivre uniquement les événements de gestion d'écriture dans AWS CloudTrail depuis le bucket S3 source et envoyer les journaux au bucket S3 de destination.
- C. Créer des notifications d'événements S3 pour déclencher une fonction Lambda pour toutes les actions sur le bucket S3 source et enregistrer directement les événements dans le bucket S3 de destination.
- D. Créer une notification d'événements S3 qui déclenche une fonction Lambda en réponse aux activités d'écriture sur le bucket S3 source. Configurer la fonction Lambda pour écrire les événements dans un flux Kinesis Data Firehose qui enregistre les journaux dans le bucket S3 désigné.

---

**34. Une grande entreprise d'analyse de données migre sa plateforme big data vers AWS en utilisant Amazon Elastic MapReduce (EMR) pour le traitement de grands ensembles de données et Amazon Aurora comme Hive metastore.**

Les ingénieurs en données doivent configurer Amazon EMR pour utiliser Amazon Aurora comme Hive metastore, afin de conserver les métadonnées même après la terminaison du cluster EMR. Quelles étapes doivent être prises ? (SÉLECTIONNER TROIS)

- A. Créer une base de données Aurora MySQL ou PostgreSQL.
- B. Modifier les groupes de sécurité pour autoriser les connexions JDBC.
- C. Configurer le catalogue de données AWS Glue.
- D. S'assurer de ne pas écrire sur la même table metastore simultanément.
- E. Définir les valeurs de configuration JDBC dans hive-site.xml.
- F. Configurer l'accès intercomptes pour EMR Serverless et le catalogue de données AWS Glue.

---

**35. Un ingénieur en données travaille sur un projet de traitement de grandes quantités de données stockées dans différents formats et emplacements.**

L'ingénieur a besoin d'une solution qui permette un développement et un débogage interactifs, supporte le traitement en temps réel et ne nécessite pas la gestion de clusters. Quelle solution AWS répondrait le mieux à ces besoins ?

- A. AWS Glue Crawlers
- B. AWS Glue Studio Job Notebooks
- C. AWS Glue Data Catalog
- D. AWS Data Pipeline

---

**36. Un ingénieur en données configure un job AWS Glue pour charger des données de plusieurs sources dans un entrepôt de données Amazon Redshift.**

L'ingénieur doit gérer les identifiants d'accès en toute sécurité. Quelle est la meilleure pratique pour gérer ces identifiants ?

- A. Passer les identifiants en tant que paramètres lors de l'exécution du job Glue.
- B. Créer un fichier de configuration contenant les identifiants et le télécharger dans un bucket Amazon S3 avec des politiques appropriées.
- C. Utiliser AWS Parameter Store pour stocker les identifiants et accorder les permissions à l'IAM du job Glue.
- D. Utiliser AWS Secrets Manager pour stocker les identifiants et accorder les permissions à l'IAM du job Glue pour récupérer les secrets.

---

**37. Une équipe d'ingénieurs en données utilisant Amazon Redshift a besoin d'une requête SQL qui renvoie une liste de données où la colonne 'category' commence par 'TAD' ou 'AGO'.**

Quelle requête SQL Redshift doit être utilisée ?

- A. `SELECT * FROM data_table WHERE category REGEXP '^TAD|^AGO';`
- B. `SELECT * FROM table WHERE category = '^TAD|AGO';`
- C. `SELECT * FROM table WHERE category = 'TAD|AGO';`
- D. `SELECT * FROM table WHERE category IN ('TAD%', 'AGO%');`

---

**38. Une entreprise héberge ses données dans cinq régions AWS distinctes, chaque région ayant son propre département des ressources humaines (RH).**

Les données des employés sont centralisées dans un lac de données Amazon S3. L'équipe d'ingénierie des données doit implémenter une stratégie qui restreint l'accès des départements RH aux enregistrements des employés de leur région respective. Quelle stratégie doit être utilisée pour restreindre l'accès avec le MOINS de surcharge opérationnelle ?

- A. Diviser les enregistrements des employés dans des buckets S3 individuels dans chaque région et configurer une politique d'accès régionale.
- B. Ajuster les rôles IAM des départements pour incorporer un filtre de données basé sur leur région.
- C. Attacher une politique IAM à chaque rôle des départements RH pour limiter l'accès aux enregistrements des employés de leur région.
- D. Utiliser AWS Lake Formation pour mettre en œuvre un contrôle d'accès granulaire. Enregistrer l'emplacement S3 avec Lake Formation et créer des filtres de données pour restreindre l'accès en fonction de la région.

---

**39. Une entreprise utilise Amazon Athena pour analyser trois ans de données partitionnées dans S3.**

Les requêtes prennent de plus en plus de temps à se terminer à mesure que les données augmentent. Comment un ingénieur en données peut-il résoudre ce problème avec le MOINS d'effort opérationnel ?

- A. Implémenter des fonctions définies par l'utilisateur (UDF) dans Athena pour ajouter une logique personnalisée aux requêtes SQL.
- B. Utiliser la projection des partitions dans Athena pour calculer directement les partitions à interroger en fonction des critères.
- C. Utiliser la réutilisation des résultats de requête dans Athena pour éviter de re-scanner les données.
- D. Utiliser la commande `Create Table As Select` (CTAS) dans Athena pour agréger et réorganiser les données dans des partitions plus larges.

---

**40. Une entreprise industrielle utilise des capteurs IoT pour surveiller les opérations dans ses secteurs d'usine.**

Les données des capteurs sont publiées dans un flux de données Amazon Kinesis Data Streams. L'entreprise souhaite surveiller les métriques en temps réel via un tableau de bord qui se met à jour en temps réel. Quelle solution permettrait d'atteindre cet objectif avec le moins de latence ?

- A. Remplacer le flux de données par Amazon Managed Streaming for Apache Kafka (MSK) et ingérer les résultats dans un bucket S3. Créer un tableau de bord Amazon QuickSight avec S3 comme source.
- B. Traiter les données des capteurs avec Amazon Managed Service for Apache Flink. Configurer un flux de livraison Data Firehose pour ingérer les données traitées directement dans Amazon Timestream et créer un tableau de bord QuickSight.
- C. Utiliser Amazon Data Firehose pour envoyer les données des capteurs dans un bucket S3, puis créer une notification S3 pour invoquer une fonction Lambda qui insère les données dans Amazon Redshift et créer un tableau de bord QuickSight.
- D. Utiliser Amazon Managed Service for Apache Flink pour le traitement des données en temps réel. Configurer un connecteur Apache Flink pour envoyer les données dans Amazon Timestream et utiliser cette base de données pour alimenter un tableau de bord Amazon Managed Grafana.

