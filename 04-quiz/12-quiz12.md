# Partie 2

**11. Une entreprise fonctionne avec deux comptes AWS – un pour la Production (Prod) et un autre pour la Sécurité.**

L'entreprise stocke des données dans divers buckets S3 répartis sur plusieurs régions AWS. Ils appliquent une politique stricte exigeant que toute action sur un bucket S3 dans le compte Prod soit enregistrée dans le compte Sécurité.

Quelle solution répondrait à cette exigence ?

- A. Créer une piste multi-régions dans AWS CloudTrail dans le compte Prod et diriger les journaux vers un bucket S3 dans le compte Sécurité.
- B. Utiliser AWS Config dans le compte Prod et envoyer des notifications des changements à un sujet SNS dans le compte Sécurité.
- C. Activer la gestion des versions sur le bucket S3 dans le compte Prod et répliquer les versions vers un bucket S3 dans le compte Sécurité.
- D. Utiliser AWS Lambda dans le compte Prod pour copier tous les nouveaux objets dans le bucket S3 vers un bucket S3 dans le compte Sécurité.

---

**12. Une entreprise utilise un vaste ensemble de données pour entraîner un modèle de machine learning (ML) à des fins d'analyse prédictive.**

L'ensemble de données contient des informations personnellement identifiables (PII) sensibles qui ne doivent pas être incluses dans le modèle ML. Les données doivent également être chiffrées à l'aide d'AWS Key Management Service (AWS KMS). 

Quelle combinaison d'étapes répondra à ces exigences de la manière la plus rentable ? (SÉLECTIONNER DEUX)

- A. Utiliser Amazon EMR pour traiter et transformer les données, en appliquant des scripts personnalisés pour masquer les PII avant l'utilisation dans le modèle ML.
- B. Envoyer les données à un cluster Amazon OpenSearch Service chiffré avec AWS KMS.
- C. Stocker les données dans un bucket Amazon S3 et activer les fonctionnalités de sécurité avec le chiffrement côté serveur KMS (SSE-KMS).
- D. Charger l'ensemble de données dans Amazon SageMaker Data Wrangler pour encoder les PII, puis procéder à l'entraînement du modèle.
- E. Mettre en œuvre AWS Glue DataBrew pour gérer l'ingestion des données et utiliser un job de recette pour dissimuler les PII dans l'ensemble de données.

---

**13. Une entreprise de santé utilise AWS Glue DataBrew pour des tâches de préparation des données.**

Un ensemble de données contenant des informations personnellement identifiables (PII) de patients doit être traité, en respectant les réglementations sur la confidentialité des données. Quelle technique de masquage des données devrait être utilisée pour remplacer les PII tout en conservant la structure et la distribution statistique des données ?

- A. Déchiffrement
- B. Substitution
- C. Chiffrement probabiliste
- D. Masquage par suppression ou nullification

---

**14. Un ingénieur en données exécute un travail AWS Glue hebdomadaire à partir d'un bucket Amazon S3, qui reçoit des données régulières d'une fonction Lambda, organisées par année, mois, et jour.**

L'ingénieur doit mettre en œuvre une solution pour mettre à jour automatiquement le catalogue de données AWS Glue dès que de nouvelles partitions sont ajoutées au bucket. Quelle solution répondra à cet objectif ?

- A. Activer l'option enableUpdateCatalog dans le script du travail Glue.
- B. Ajouter un code dans la fonction Lambda pour invoquer automatiquement l'API create_partition de Boto3 dans AWS Glue.
- C. Configurer un crawler AWS Glue planifié qui s'exécute quotidiennement.
- D. Exécuter la commande MSCK REPAIR TABLE depuis l'éditeur de requêtes Amazon Athena après l'ajout de nouvelles données.

---

**15. Une entreprise financière utilise une base de données Amazon Aurora pour son système d'approbation de prêts.**

Lorsqu'une demande de prêt est mise à jour, divers systèmes de traitement doivent être déclenchés. Quelle option répondrait le mieux aux exigences ?

- A. Créer une procédure stockée ou une fonction native qui invoque une fonction Lambda lors de la mise à jour du prêt. Configurer Lambda pour envoyer l'événement à une file Amazon SQS.
- B. Configurer une souscription d'événements RDS pour déclencher une fonction Lambda lors de la mise à jour de la demande de prêt. 
- C. Créer une règle Amazon EventBridge pour détecter les changements dans la base de données. Configurer la règle pour déclencher une fonction Lambda.
- D. Configurer Amazon Aurora pour publier des messages directement sur un sujet SNS lors de l'approbation d'un prêt.

---

**16. Une équipe d'ingénieurs en données développe un pipeline ETL avec un lac de données basé sur Amazon S3 comme source.**

Ils découvrent des informations sensibles. Quelle combinaison d'étapes permettrait d'identifier et de masquer les informations personnellement identifiables (PII) avec le MOINS d'effort de développement ? (SÉLECTIONNER DEUX)

- A. Utiliser AWS Macie pour identifier et masquer les PII.
- B. Créer un job Glue dans AWS Glue Studio avec la transformation Detect PII pour identifier et masquer les PII.
- C. Utiliser AWS Step Functions pour orchestrer le flux de traitement des données.
- D. Mettre en place un flux de livraison Amazon Data Firehose avec AWS Lambda pour le masquage des PII.
- E. Créer une règle AWS Glue Data Quality pour déterminer et masquer les PII.

---

**17. Une entreprise héberge son application sur des instances Amazon EC2 qui se connectent à une base de données Amazon RDS pour Microsoft SQL Server.**

Ils veulent minimiser l'exposition de leurs identifiants de base de données. Quelle solution répondrait à ce besoin ?

- A. Utiliser l'authentification par AWS Identity and Access Management (IAM) pour Amazon RDS.
- B. Utiliser AWS Secrets Manager pour stocker et faire tourner automatiquement les identifiants tous les 30 jours.
- C. Utiliser les paramètres avancés d'AWS Systems Manager Parameter Store pour la rotation des identifiants.
- D. Utiliser AWS IAM Identity Center pour authentifier les instances EC2.

---

**18. Une équipe d'ingénieurs en données cherche à améliorer les performances de leurs workloads d'analyse de données.**

Ils utilisent intensivement Amazon Athena pour interroger des enregistrements stockés dans Amazon S3. Quelle méthode serait la PLUS efficace pour optimiser les performances des requêtes Athena ?

- A. Mettre à jour fréquemment l'indexation des données dans Amazon S3 pour améliorer les vitesses de récupération de données.
- B. Exécuter une requête CREATE TABLE AS SELECT (CTAS) dans Athena pour convertir les résultats en formats de stockage efficaces comme Parquet ou ORC.
- C. Utiliser une requête fédérée dans Amazon Athena pour interroger des sources de données disparates.
- D. Activer la fonctionnalité de réutilisation des résultats de requêtes courantes dans Athena.

---

**19. Une entreprise basée aux Philippines gère des commandes mondiales placées dans une file Amazon SQS.**

Un workflow AWS Step Functions ne doit traiter que les commandes internationales. Quelle option satisferait les exigences avec le MOINS d'effort de développement ?

- A. Créer une règle Amazon EventBridge avec un modèle d'événement correspondant aux actions SendMessage SQS.
- B. Filtrer les commandes en fonction de la longueur de la file SQS.
- C. Créer une fonction AWS Lambda avec un mappage de source d'événements SQS.
- D. Créer un pipeline dans Amazon EventBridge Pipes avec la file SQS comme source.

---

**20. Une entreprise exécute une solution d'entrepôt de données dans un cluster Amazon Redshift.**

Un ingénieur exécute une requête qui génère une erreur avec la fonction `current_schema()`. Quelle est la cause la plus probable de ce problème ?

- A. Les requêtes faisant référence à des tables du catalogue s'exécutent exclusivement sur le nœud de calcul.
- B. La fonction `current_schema()` ne peut pas être exécutée avec des tables système `pg_table_def` résidant sur les nœuds de calcul.
- C. Les requêtes qui utilisent des fonctions définies par l'utilisateur et des tables de catalogue peuvent uniquement s'exécuter sur le nœud leader.
- D. La fonction `current_schema()` est exclusive au nœud de calcul et ne peut pas s'exécuter avec des tables utilisateur.
